{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\raven\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python37\\\\site-packages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class act_model(nn.Module):\n",
    "    def __init__(self,inp,hidden,output):\n",
    "        super(act_model, self).__init__()\n",
    "        self.fc1 = nn.Linear(inp, hidden, bias=True)\n",
    "        self.fc2 = nn.Linear(hidden, hidden, bias=True)\n",
    "        self.fc3 = nn.Linear(hidden, output, bias=True)\n",
    "        self.fc8 = nn.Tanh()\n",
    "        self.fc9 = nn.Sigmoid()\n",
    "        self.fc10 = nn.Softmax()\n",
    "        self.fc11 = nn.ReLU()\n",
    "        self.fc12 = nn.LeakyReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.fc12(self.fc1(x))\n",
    "        out = self.fc12(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_dis_reward(reward_list,gamma):\n",
    "    reward_list.reverse()\n",
    "    dis_reward_list = [reward_list[0]]\n",
    "    for i in range(1,len(reward_list)):\n",
    "        dis_reward_list.append(reward_list[i] + gamma*dis_reward_list[i-1])\n",
    "    dis_reward_list.reverse()\n",
    "    return dis_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = 4\n",
    "hid = 24\n",
    "out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = act_model(inp,hid,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_list = []\n",
    "loss_list = []\n",
    "mse = nn.MSELoss()\n",
    "bce = nn.BCELoss()\n",
    "nll = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "gamma = .9\n",
    "epsilon = .2\n",
    "epochs = 1000\n",
    "total_reward_list = []\n",
    "tot_loss = []\n",
    "buffer = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    observation = env.reset()\n",
    "    observation_list = []\n",
    "    old_q_list = []\n",
    "    reward_list = []\n",
    "    max_q_list = []\n",
    "    max_q_act_ind_list = []\n",
    "    tot_reward = 0\n",
    "    for t in range(100):\n",
    "        observation_list.append(observation)\n",
    "        \n",
    "        q_action = model(torch.FloatTensor(observation))   #Q(s)\n",
    "        \n",
    "        max_q_act_ind = np.argmax(q_action.detach().numpy())  #Q(s,a)\n",
    "        \n",
    "        observation, reward, done, info = env.step(max_q_act_ind)\n",
    "        tot_reward += reward\n",
    "        \n",
    "        max_q_list.append(q_action[max_q_act_ind])\n",
    "        max_q_act_ind_list.append(max_q_act_ind)\n",
    "        reward_list.append(reward)\n",
    "\n",
    "        if done:\n",
    "            reward_list[-1] = -10\n",
    "            print(\"Epoch: {} timesteps: {}  Reward: {} \".format(epoch,t+1,tot_reward))\n",
    "            break       \n",
    "    \n",
    "    dis_reward_list = temp_dis_reward(reward_list,gamma)\n",
    "    dis_old_q_list = temp_dis_reward(max_q_list,gamma)\n",
    "    max_q_list = temp_dis_reward(max_q_list,gamma)\n",
    "    \n",
    "    #print(dis_reward_list)\n",
    "    ten_dis_rew_list = []\n",
    "    ten_q_list = []\n",
    "    for i in range(len(dis_reward_list)):\n",
    "        ind = max_q_act_ind_list[i]\n",
    "\n",
    "        one_hot_reward = torch.zeros(2)\n",
    "        one_hot_reward[ind] = dis_reward_list[i]\n",
    "\n",
    "        one_hot_q = torch.zeros(2)\n",
    "        one_hot_q[ind] = max_q_list[i]\n",
    "\n",
    "        ten_dis_rew_list.append(one_hot_reward)\n",
    "        ten_q_list.append(one_hot_q)\n",
    "    \n",
    "    \n",
    "    ten_dis_rew_list = torch.stack(ten_dis_rew_list)\n",
    "    ten_q_list = torch.stack(ten_q_list)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "    loss = mse(ten_dis_rew_list,ten_q_list)\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step() \n",
    "        \n",
    "    total_reward_list.append(tot_reward)   \n",
    "    tot_loss.append(loss.detach())\n",
    "       \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(len(total_reward_list))],total_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(len(tot_loss))],tot_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(total_reward_list)/len(total_reward_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "gamma = .9\n",
    "total_reward = 0\n",
    "total_reward_list = []\n",
    "observation = env.reset()\n",
    "for t in range(100):\n",
    "    env.render()\n",
    "    old_action = model(torch.FloatTensor(observation))\n",
    "    old_max_act_ind = np.argmax(old_action.detach().numpy())\n",
    "\n",
    "    observation, reward, done, info = env.step(old_max_act_ind)\n",
    "    total_reward+=reward\n",
    "    if done:\n",
    "        print(\"{} timesteps. Reward: {} \".format(t+1,total_reward))\n",
    "        break       \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
